{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import InterfacePreprocessing as IntPre\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.ndimage import imread\n",
    "from scipy.misc import imresize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global raw_only, image_size, num_layer\n",
    "raw_only = False\n",
    "raw_image_size = 250\n",
    "image_size= 88\n",
    "if raw_only:\n",
    "    num_layer = 1\n",
    "else:\n",
    "    num_layer = 6\n",
    "\n",
    "p_matching = 0.5\n",
    "num_filter_1 = 20\n",
    "num_filter_2 = 40\n",
    "num_filter_3 = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############### #\n",
    "#   Helper Func   #\n",
    "# ############### #\n",
    "\n",
    "'''\n",
    "# Create the same image merge with maximum 3 kind of pre-processing\n",
    "def preprocessing_bare(image, size):\n",
    "    \n",
    "    if raw_only:\n",
    "        image_temp = np.zeros((size,size,1))\n",
    "        image = imresize(image, [size, size])\n",
    "        image_temp[:,:,0] = image\n",
    "    else:\n",
    "        image_temp = np.zeros((size,size,4))\n",
    "        image = imresize(image, [size, size])\n",
    "        image_temp[:,:,0] = image # 180*180\n",
    "        image_temp[:,:,1] = imresize(IntPre.frequency(image, step=5), [size, size])\n",
    "        image_temp[:,:,2] = imresize(np.abs(IntPre.orientation(image, coherence=False)), [size, size])\n",
    "        image_temp[:,:,3] = imresize(IntPre.variance(image, block=5), [size, size])\n",
    "        \n",
    "    return image_temp\n",
    "\n",
    "# To load images like CASIA (no matching etc...)\n",
    "def load_vrac(path):\n",
    "    images = dict()\n",
    "\n",
    "    for f in os.listdir(path):\n",
    "        ID = f[:-4]\n",
    "        image = imread(path+f).astype(np.int32)\n",
    "        image_temp = preprocessing_bare(image, raw_image_size)\n",
    "        images[ID] = image_temp\n",
    "    return images\n",
    "\n",
    "# To load images organised by pairs (of match and mismatch)\n",
    "def load_pairs(path_match, path_mismatch):\n",
    "    return load_vrac(path_match), load_vrac(path_mismatch)\n",
    "\n",
    "# If images are loaded with \"load_pairs\"\n",
    "def generate_batch_pairs(images_match, images_mismatch, num, image_size):\n",
    "        \n",
    "    x = np.zeros((num, image_size*image_size*num_layer))\n",
    "    x_p = np.zeros((num, image_size*image_size*num_layer))\n",
    "    y = np.zeros((num, 1))\n",
    "    match = int(np.round(num*0.5))\n",
    "    mis_match = num - match\n",
    "\n",
    "    keys_mismatch = images_mismatch.keys()\n",
    "    for i in range(mis_match):\n",
    "        ID = random.choice(keys_mismatch)[:-1]\n",
    "        img = images_mismatch[ID + '0']\n",
    "        x1 = img[:88,:88,:]\n",
    "        x2 = img[4:92,:88,:]\n",
    "        x[i,:] = np.reshape(x1, (1,image_size*image_size*num_layer))\n",
    "        x_p[i,:] = np.reshape(x2, (1,image_size*image_size*num_layer))\n",
    "        y[i] = 0\n",
    "\n",
    "    keys_match = images_match.keys()\n",
    "    for i in range(match):\n",
    "        ID = random.choice(keys_match)[:-1]\n",
    "        img = images_match[ID + '0']\n",
    "        x1 = img[:88,:88,:]\n",
    "        x2 = img[88:176,:88,:]      \n",
    "        x[mis_match+i,:] = np.reshape(x1, (1,image_size*image_size*num_layer))\n",
    "        x_p[mis_match+i,:] = np.reshape(x2, (1,image_size*image_size*num_layer))\n",
    "        y[mis_match+i] = 1\n",
    "        \n",
    "    return [x, x_p, y]\n",
    "'''\n",
    "\n",
    "def load_pairs_from_preprocessed(match_path, mismatch_path):\n",
    "    images_match = dict()\n",
    "\n",
    "    for ID in range(94):\n",
    "        for pic in range(2):\n",
    "            image_temp = np.zeros((image_size,image_size,num_layer))\n",
    "            for layer in range(num_layer):\n",
    "                fname = str(ID+1)+'_'+str(pic)+'_'+str(layer)+'.png'\n",
    "                image_temp[:,:,layer] = imresize(imread(match_path+fname).astype(np.int32),[image_size,image_size])\n",
    "                \n",
    "            images_match[str(ID+1)+'_'+str(pic)] = image_temp\n",
    "    \n",
    "    images_mismatch = dict()\n",
    "\n",
    "    for ID in range(94):\n",
    "        for pic in range(2):\n",
    "            image_temp = np.zeros((image_size,image_size,num_layer))\n",
    "            for layer in range(num_layer):\n",
    "                fname = str(ID+1)+'_'+str(pic)+'_'+str(layer)+'.png'\n",
    "                image_temp[:,:,layer] = imresize(imread(mismatch_path+fname).astype(np.int32),[image_size,image_size])\n",
    "            images_mismatch[str(ID+1)+'_'+str(pic)] = image_temp\n",
    "            \n",
    "    return images_match, images_mismatch\n",
    "\n",
    "def generate_batch_pairs_from_preprocessed(images_match, images_mismatch, num, image_size):\n",
    "        \n",
    "    x = np.zeros((num, image_size*image_size*num_layer))\n",
    "    x_p = np.zeros((num, image_size*image_size*num_layer))\n",
    "    y = np.ones((num, 1))*-1\n",
    "    match = int(np.round(num*0.5))\n",
    "    mis_match = num - match\n",
    "\n",
    "    keys_mismatch = images_mismatch.keys()\n",
    "    for i in range(mis_match):\n",
    "        ID = random.choice(keys_mismatch)[:-1]\n",
    "        img = images_mismatch[ID + '0']\n",
    "        img_p = images_mismatch[ID + '1']\n",
    "        x[i,:] = np.reshape(img, (1,image_size*image_size*num_layer))\n",
    "        x_p[i,:] = np.reshape(img_p, (1,image_size*image_size*num_layer))\n",
    "        y[i] = 0\n",
    "\n",
    "    keys_match = images_match.keys()\n",
    "    for i in range(match):\n",
    "        ID = random.choice(keys_match)[:-1]\n",
    "        img = images_match[ID + '0']\n",
    "        img_p = images_match[ID + '1']  \n",
    "        x[mis_match+i,:] = np.reshape(img, (1,image_size*image_size*num_layer))\n",
    "        x_p[mis_match+i,:] = np.reshape(img_p, (1,image_size*image_size*num_layer))\n",
    "        y[mis_match+i] = 1\n",
    "        \n",
    "    return [x, x_p, y]\n",
    "\n",
    "# ################## #\n",
    "# Helper Func for tf #\n",
    "# ################## #\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.3, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def assist_variable(shape):\n",
    "    initial = tf.constant(0.3, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride=[1, 1, 1, 1]):\n",
    "    return tf.nn.conv2d(x, W, strides=stride, padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "def sigm(x):\n",
    "    return tf.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ################## #\n",
    "#     Old Version    #\n",
    "# ################## #\n",
    "\n",
    "# images_match, images_mismatch = load_pairs(\"./PAIRS/MATCHED/CROP/\", \"./PAIRS/MISMATCHED/CROP/\")\n",
    "\n",
    "# for key in images_match.keys():\n",
    "#     images_match[key][:,:,2] = np.abs(IntPre.orientation(images_match[key][:,:,0], coherence=False))\n",
    "    \n",
    "# for key in images_mismatch.keys():\n",
    "#     images_mismatch[key][:,:,2] = np.abs(IntPre.orientation(images_mismatch[key][:,:,0], coherence=False))\n",
    "\n",
    "# ################## #\n",
    "#     New Version    #\n",
    "# ################## #\n",
    "images_match, images_mismatch = load_pairs_from_preprocessed(\"./PreProcessed/Match/\", \"./PreProcessed/MisMatch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key = images_match.keys()[0]\n",
    "plt.figure(figsize=(17,5))\n",
    "ax = plt.subplot(1,6,1)\n",
    "ax.imshow(images_match[key][:,:,0])\n",
    "ax = plt.subplot(1,6,2)\n",
    "ax.imshow(images_match[key][:,:,1])\n",
    "ax = plt.subplot(1,6,3)\n",
    "ax.imshow(images_match[key][:,:,2])\n",
    "ax = plt.subplot(1,6,4)\n",
    "ax.imshow(images_match[key][:,:,3])\n",
    "ax = plt.subplot(1,6,5)\n",
    "ax.imshow(images_match[key][:,:,4])\n",
    "ax = plt.subplot(1,6,6)\n",
    "ax.imshow(images_match[key][:,:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new TF session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# These two are input images\n",
    "x = tf.placeholder(tf.float32, shape=[None, image_size*image_size*num_layer])\n",
    "x_p = tf.placeholder(tf.float32, shape=[None, image_size*image_size*num_layer])\n",
    "\n",
    "# y_ is just a value 0(match) or 1(no match) for the two input images\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# Dropout coefficient\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Shared Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([7, 7, num_layer, num_filter_1])\n",
    "b_conv1 = bias_variable([num_filter_1])\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, num_filter_1, num_filter_2])\n",
    "b_conv2 = bias_variable([num_filter_2])\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, num_filter_2, num_filter_3])\n",
    "b_conv3 = bias_variable([num_filter_3])\n",
    "\n",
    "W_conv1_p = weight_variable([7, 7, num_layer, num_filter_1])\n",
    "b_conv1_p = bias_variable([num_filter_1])\n",
    "\n",
    "W_conv2_p = weight_variable([5, 5, num_filter_1, num_filter_2])\n",
    "b_conv2_p = bias_variable([num_filter_2])\n",
    "\n",
    "W_conv3_p = weight_variable([5, 5, num_filter_2, num_filter_3])\n",
    "b_conv3_p = bias_variable([num_filter_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Sides of Siamese Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ############### #\n",
    "#      Side 1     #\n",
    "# ############### #\n",
    "\n",
    "# Input Image\n",
    "x_image = tf.reshape(x, [-1,image_size,image_size,num_layer])\n",
    "\n",
    "# First Conv Layer - after maxpool 44*44\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# Second Conv Layer - after maxpool 22*22\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# Third Conv Layer - after maxpool 11*11\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "# Final Data Processing Step\n",
    "z = tf.reshape(h_pool3, [-1,image_size/4*image_size/4*num_filter_3])\n",
    "# z = tf.reshape(h_pool3, [-1,4*4*256])\n",
    "z_norm = tf.pow(tf.reduce_sum(tf.pow(z, 2), reduction_indices=1),0.5)\n",
    "\n",
    "# ############### #\n",
    "#      Side 2     #\n",
    "# ############### #\n",
    "\n",
    "# Input Image\n",
    "x_image_p = tf.reshape(x_p, [-1,image_size,image_size,num_layer])\n",
    "\n",
    "# First Conv Layer - after maxpool 44*44\n",
    "h_conv1_p = tf.nn.relu(conv2d(x_image_p, W_conv1_p) + b_conv1_p)\n",
    "h_pool1_p = max_pool_2x2(h_conv1_p)\n",
    "\n",
    "# Second Conv Layer - after maxpool 22*22\n",
    "h_conv2_p = tf.nn.relu(conv2d(h_pool1_p, W_conv2_p) + b_conv2_p)\n",
    "h_pool2_p = max_pool_2x2(h_conv2_p)\n",
    "\n",
    "# Third Conv Layer - after maxpool 11*11\n",
    "h_conv3_p = tf.nn.relu(conv2d(h_pool2_p, W_conv3_p) + b_conv3_p)\n",
    "h_pool3_p = max_pool_2x2(h_conv3_p)\n",
    "\n",
    "# Final Data Processing Step\n",
    "z_p = tf.reshape(h_pool3_p, [-1,image_size/4*image_size/4*num_filter_3])\n",
    "# z_p = tf.reshape(h_pool3_p, [-1,4*4*256])\n",
    "z_p_norm = tf.pow(tf.reduce_sum(tf.pow(z_p, 2), reduction_indices=1),0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fc\n",
    "\n",
    "W_fc1 = weight_variable([image_size/4*image_size/4*num_filter_3, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, image_size/4*image_size/4*num_filter_3])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "\n",
    "distance = tf.div(tf.reduce_sum(z*z_p, reduction_indices=1), z_norm*z_p_norm)\n",
    "cross_entropy = tf.pow(tf.reduce_sum(tf.pow(distance, 2), reduction_indices=1), 0.5)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(100):\n",
    "    batch = generate_batch_pairs_from_preprocessed(images_match, images_mismatch, 50, image_size)\n",
    "    if i%10 == 0:\n",
    "#         train_accuracy = accuracy.eval(feed_dict={x:batch[0], x_p:batch[1], y_: batch[2], keep_prob: 1.0})\n",
    "#         print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        c_e = cross_entropy.eval(feed_dict={x:batch[0], x_p:batch[1], y_: batch[2], keep_prob: 1.0})\n",
    "        print c_e\n",
    "    train_step.run(feed_dict={x:batch[0], x_p:batch[1], y_: batch[2], keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distance = tf.div(tf.reduce_sum(z*z_p, reduction_indices=1), z_norm*z_p_norm)\n",
    "batch = generate_batch_pairs_from_preprocessed(images_match, images_mismatch, 50, image_size)\n",
    "\n",
    "prediction = []\n",
    "results = distance.eval(feed_dict={x:batch[0], x_p:batch[1], y_: batch[2], keep_prob: 1.0})\n",
    "# for i in range(50):\n",
    "#     if results[0][i]<0:\n",
    "#         prediction.append(1)\n",
    "#     else:\n",
    "#         prediction.append(0)\n",
    "    \n",
    "# accuracy = np.mean(np.equal(prediction, np.concatenate(batch[2])))\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the test on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_mnist(n, mode='train'):\n",
    "    x, x_p = np.zeros((n, 784)), np.zeros((n, 784))\n",
    "    y = np.zeros((n,1))\n",
    "    \n",
    "    matched = 0\n",
    "    mismatched = np.round(n/2)\n",
    "    while matched < np.round(n/2):\n",
    "        if mode == 'train':\n",
    "            batch = mnist.train.next_batch(2, )\n",
    "        elif mode == 'test':\n",
    "            batch = mnist.test.next_batch(2)\n",
    "        else:\n",
    "            batch = mnist.validation.next_batch(2)\n",
    "        \n",
    "        if np.argmax(batch[1],axis=1)[0] == np.argmax(batch[1],axis=1)[1]:\n",
    "            x[matched,:] = batch[0][0,:]\n",
    "            x_p[matched,:] = batch[0][1,:]\n",
    "            y[matched] = 1\n",
    "            matched += 1\n",
    "            \n",
    "        elif mismatched > 0:\n",
    "            x[-mismatched,:] = batch[0][0,:]\n",
    "            x_p[-mismatched,:] = batch[0][1,:]\n",
    "            mismatched -= 1\n",
    "            \n",
    "    return [x, x_p, y]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = generate_batch_mnist(2)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    ax.imshow(np.reshape(batch[0][i,:],(28,28)))\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    ax.imshow(np.reshape(batch[1][i,:],(28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "distance = tf.div(tf.reduce_sum(z*z_p, reduction_indices=1), z_norm*z_p_norm) - tf.transpose(y_)\n",
    "cross_entropy = tf.pow(tf.reduce_sum(tf.pow(distance, 2), reduction_indices=1),0.5)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "rounded = tf.round(cross_entropy*-1)\n",
    "correct_prediction = tf.equal(rounded,tf.transpose(y_))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(10000):\n",
    "    mnist_batch = generate_batch_mnist(50, 'train')\n",
    "    if i%500 == 0:\n",
    "        c_e = cross_entropy.eval(feed_dict={x:mnist_batch[0], x_p:mnist_batch[1], y_: mnist_batch[2], keep_prob: 1.0})\n",
    "        print c_e\n",
    "#         print \"Accuracy %g\" %accuracy.eval(feed_dict={x:mnist_batch[0], x_p:mnist_batch[1], y_: mnist_batch[2], keep_prob: 1.0})\n",
    "\n",
    "    train_step.run(feed_dict={x:mnist_batch[0], x_p:mnist_batch[1], y_: mnist_batch[2], keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit on one side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "# 1 - output 14*14*20\n",
    "\n",
    "W_conv1 = weight_variable([7, 7, 1, 20])\n",
    "b_conv1 = bias_variable([20])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# 2 - output 7*7*80\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 20, 40])\n",
    "b_conv2 = bias_variable([40])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# 3 - output 4*4*320\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 40, 80])\n",
    "b_conv3 = bias_variable([80])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "# fc\n",
    "\n",
    "W_fc1 = weight_variable([4*4*80, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 4*4*80])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the overall performance through test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "trainlab = tf.argmax(y_,1)\n",
    "predlab = tf.argmax(y_conv,1)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(1000):\n",
    "    batch = mnist.test.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "#         print trainlab.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})[0:10], predlab.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})[0:10]\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "# 1 - output 14*14*20\n",
    "\n",
    "W_conv1 = weight_variable([7, 7, 1, 20])\n",
    "b_conv1 = bias_variable([20])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# 2 - output 7*7*80\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 20, 40])\n",
    "b_conv2 = bias_variable([40])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# 3 - output 4*4*320\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 40, 80])\n",
    "b_conv3 = bias_variable([80])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "# fc\n",
    "\n",
    "W_fc1 = weight_variable([4*4*80, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 4*4*80])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver({\"W1\": W_conv1, \"W2\": W_conv2,\"W3\": W_conv3,\n",
    "                        \"b1\": b_conv1,\"b2\": b_conv2,\"b3\": b_conv3,\n",
    "                        \"W_fc1\": W_fc1, \"b_fc1\": b_fc1,\n",
    "                        \"W_fc2\": W_fc2, \"b_fc2\": b_fc2})\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Do some work with the model.\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    for i in range(1000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "    \n",
    "    # Save the variables to disk.\n",
    "    \n",
    "    save_path = saver.save(sess, \"one_sided.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create some variables.\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "# 1 - output 14*14*20\n",
    "\n",
    "W_conv1 = weight_variable([7, 7, 1, 20])\n",
    "b_conv1 = bias_variable([20])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# 2 - output 7*7*80\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 20, 40])\n",
    "b_conv2 = bias_variable([40])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# 3 - output 4*4*320\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 40, 80])\n",
    "b_conv3 = bias_variable([80])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "# fc\n",
    "\n",
    "W_fc1 = weight_variable([4*4*80, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 4*4*80])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver({\"W1\": W_conv1, \"W2\": W_conv2,\"W3\": W_conv3,\n",
    "                        \"b1\": b_conv1,\"b2\": b_conv2,\"b3\": b_conv3,\n",
    "                        \"W_fc1\": W_fc1, \"b_fc1\": b_fc1,\n",
    "                        \"W_fc2\": W_fc2, \"b_fc2\": b_fc2})\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    # Do some work with the model.\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    \n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"one_sided.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    batch = generate_batch_mnist_train(50)\n",
    "    left = batch[0]\n",
    "    right = batch[1]\n",
    "    label = batch[2]\n",
    "    left_output = y_conv.eval(feed_dict={x:left, y_: mnist.train.next_batch(50)[1], keep_prob: 1.0})\n",
    "    right_output = y_conv.eval(feed_dict={x:right, y_: mnist.train.next_batch(50)[1], keep_prob: 1.0})\n",
    "    \n",
    "    # plt.plot(np.sum(left_output*right_output, axis=1),'.')\n",
    "#     fpr, tpr, thresholds = roc_curve(label, -np.linalg.norm(left_output-right_output, axis=1))\n",
    "    \n",
    "#     fpr, tpr, thresholds = roc_curve(label, \n",
    "#                                      np.sum(left_output*right_output/(np.linalg.norm(left_output)*np.linalg.norm(right_output)), \n",
    "#                                             axis=1))\n",
    "\n",
    "    cd = np.zeros(50)\n",
    "    diff = left_output-right_output\n",
    "    for i in range(50):\n",
    "        cd[i] = np.linalg.norm(diff[i])*label[i]+(1-label[i])*np.square(np.max([0,5-np.linalg.norm(diff[i])]))\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(label, -cd)\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.title('Output Layer with AUC %.3f' %auc(fpr, tpr))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "#     batch = generate_batch_mnist(50)\n",
    "#     left = batch[0]\n",
    "#     right = batch[1]\n",
    "#     label = batch[2]\n",
    "#     left_output = h_fc1.eval(feed_dict={x:left, y_: mnist.train.next_batch(50)[1], keep_prob: 1.0})\n",
    "#     right_output = h_fc1.eval(feed_dict={x:right, y_: mnist.train.next_batch(50)[1], keep_prob: 1.0})\n",
    "\n",
    "# #     fpr, tpr, thresholds = roc_curve(label, \n",
    "# #                                      np.sum(left_output*right_output/(np.linalg.norm(left_output)*np.linalg.norm(right_output)), \n",
    "# #                                             axis=1))\n",
    "\n",
    "# #     fpr, tpr, thresholds = roc_curve(label,-np.linalg.norm(left_output-right_output, axis=1))\n",
    "    \n",
    "#     cd = np.zeros(50)\n",
    "#     diff = left_output-right_output\n",
    "#     for i in range(50):\n",
    "#         cd[i] = np.linalg.norm(diff[i])*label[i]+(1-label[i])*np.square(np.max([0,1-np.linalg.norm(diff[i])]))\n",
    "    \n",
    "#     fpr, tpr, thresholds = roc_curve(label, cd)\n",
    "    \n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr, tpr)\n",
    "#     plt.plot([0, 1], [0, 1], 'k--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.title('Fully Connected with AUC %.3f' %auc(fpr, tpr))\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "    \n",
    "#     batch = generate_batch_mnist(50)\n",
    "#     left = batch[0]\n",
    "#     right = batch[1]\n",
    "#     label = batch[2]\n",
    "#     left_output = h_pool3_flat.eval(feed_dict={x:left, y_: mnist.train.next_batch(50)[1], keep_prob: 1.0})\n",
    "#     right_output = h_pool3_flat.eval(feed_dict={x:right, y_: mnist.train.next_batch(50)[1], keep_prob: 1.0})\n",
    "\n",
    "# #     fpr, tpr, thresholds = roc_curve(label, \n",
    "# #                                      np.sum(left_output*right_output/(np.linalg.norm(left_output)*np.linalg.norm(right_output)), \n",
    "# #                                             axis=1))\n",
    "    \n",
    "    \n",
    "#     cd = np.zeros(50)\n",
    "#     diff = left_output-right_output\n",
    "#     for i in range(50):\n",
    "#         cd[i] = np.linalg.norm(diff[i])*label[i]+(1-label[i])*np.square(np.max([0,5-np.linalg.norm(diff[i])]))\n",
    "    \n",
    "#     fpr, tpr, thresholds = roc_curve(label, cd)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr, tpr)\n",
    "#     plt.plot([0, 1], [0, 1], 'k--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.title('Convolutional Output with AUC %.3f' %auc(fpr, tpr))\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd = np.zeros(50)\n",
    "diff = left_output-right_output\n",
    "for i in range(50):\n",
    "    cd[i] = np.linalg.norm(diff[i])*label[i]+(1-label[i])*np.square(np.max([0,5-np.linalg.norm(diff[i])]))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(label, cd-0.5)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.title('Output Layer with AUC %.3f' %auc(fpr, tpr))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd-.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
